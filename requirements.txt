vllm # Great for fast, memory-efficient inference
torch
transformers # off-the-shelf from huggingface. Contains llama 3
alpaca_eval @ git+https://github.com/nelson-liu/alpaca_eval.git@forward_kwargs_to_vllm
xopen
tqdm
huggingface_hub # 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
pandas